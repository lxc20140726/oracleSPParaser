#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½æµ‹è¯•
"""

import pytest
import time
import statistics
import sys
import gc
from pathlib import Path
from typing import List, Dict, Any
import memory_profiler
import psutil
import os
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

@pytest.mark.performance
class TestAnalysisPerformance:
    """åˆ†ææ€§èƒ½æµ‹è¯•"""
    
    def test_single_analysis_performance(self, oracle_sp_analyzer, sample_stored_procedure):
        """æµ‹è¯•å•æ¬¡åˆ†ææ€§èƒ½"""
        # é¢„çƒ­
        oracle_sp_analyzer.analyze(sample_stored_procedure)
        
        # æ€§èƒ½æµ‹è¯•
        execution_times = []
        for _ in range(10):
            start_time = time.time()
            result = oracle_sp_analyzer.analyze(sample_stored_procedure)
            end_time = time.time()
            
            execution_times.append(end_time - start_time)
            assert result is not None
        
        # ç»Ÿè®¡åˆ†æ
        avg_time = statistics.mean(execution_times)
        median_time = statistics.median(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)
        
        print(f"\nå•æ¬¡åˆ†ææ€§èƒ½ç»Ÿè®¡:")
        print(f"å¹³å‡æ—¶é—´: {avg_time:.4f}ç§’")
        print(f"ä¸­ä½æ—¶é—´: {median_time:.4f}ç§’")
        print(f"æœ€å°æ—¶é—´: {min_time:.4f}ç§’")
        print(f"æœ€å¤§æ—¶é—´: {max_time:.4f}ç§’")
        
        # æ€§èƒ½æ–­è¨€
        assert avg_time < 2.0, f"å¹³å‡åˆ†ææ—¶é—´è¿‡é•¿: {avg_time}ç§’"
        assert max_time < 5.0, f"æœ€å¤§åˆ†ææ—¶é—´è¿‡é•¿: {max_time}ç§’"
    
    @pytest.mark.slow
    def test_thousand_line_procedure_performance(self, oracle_sp_analyzer):
        """æµ‹è¯•åƒè¡Œå­˜å‚¨è¿‡ç¨‹æ€§èƒ½"""
        print("\nğŸ”¥ å¼€å§‹ç”Ÿæˆåƒè¡Œå­˜å‚¨è¿‡ç¨‹...")
        
        # ç”ŸæˆçœŸæ­£çš„åƒè¡Œå­˜å‚¨è¿‡ç¨‹
        thousand_line_sp_parts = [
            "CREATE OR REPLACE PROCEDURE thousand_line_performance_test(",
            "    p_batch_size IN NUMBER DEFAULT 100,",
            "    p_start_date IN DATE DEFAULT SYSDATE - 365,",
            "    p_end_date IN DATE DEFAULT SYSDATE,",
            "    p_department_filter IN VARCHAR2 DEFAULT NULL,",
            "    p_process_mode IN VARCHAR2 DEFAULT 'STANDARD',",
            "    p_result_count OUT NUMBER,",
            "    p_error_count OUT NUMBER,",
            "    p_execution_time OUT NUMBER",
            ") AS",
            "    -- å£°æ˜å˜é‡éƒ¨åˆ†",
            "    v_start_time TIMESTAMP := SYSTIMESTAMP;",
            "    v_batch_counter NUMBER := 0;",
            "    v_total_processed NUMBER := 0;",
            "    v_error_counter NUMBER := 0;",
            "    v_temp_count NUMBER;",
            "    v_avg_salary NUMBER;",
            "    v_max_salary NUMBER;",
            "    v_min_salary NUMBER;",
            "    v_department_name VARCHAR2(200);",
            "    v_employee_count NUMBER;",
            "    v_bonus_amount NUMBER;",
            "    v_commission_rate NUMBER;",
            "    v_performance_rating VARCHAR2(20);",
            "    v_last_promotion_date DATE;",
            "    v_years_of_service NUMBER;",
            "    v_current_level NUMBER;",
            "    v_next_level NUMBER;",
            "    v_salary_adjustment NUMBER;",
            "    v_budget_allocation NUMBER;",
            "    v_cost_center VARCHAR2(50);",
            "    v_region_code VARCHAR2(10);",
            "    v_branch_id NUMBER;",
            "    v_division_code VARCHAR2(20);",
            "    v_project_count NUMBER;",
            "    v_training_hours NUMBER;",
            "    v_certification_count NUMBER;",
            "",
            "    -- æ¸¸æ ‡å£°æ˜",
            "    CURSOR c_employees IS",
            "        SELECT emp.employee_id, emp.first_name, emp.last_name, emp.email,",
            "               emp.salary, emp.hire_date, emp.department_id, emp.manager_id,",
            "               emp.job_id, emp.commission_pct, emp.phone_number,",
            "               dept.department_name, dept.location_id, dept.manager_id as dept_manager,",
            "               job.job_title, job.min_salary, job.max_salary,",
            "               loc.city, loc.state_province, loc.country_id,",
            "               mgr.first_name as mgr_first_name, mgr.last_name as mgr_last_name",
            "        FROM employees emp",
            "        INNER JOIN departments dept ON emp.department_id = dept.department_id",
            "        INNER JOIN jobs job ON emp.job_id = job.job_id",
            "        LEFT JOIN locations loc ON dept.location_id = loc.location_id",
            "        LEFT JOIN employees mgr ON emp.manager_id = mgr.employee_id",
            "        WHERE emp.hire_date BETWEEN p_start_date AND p_end_date",
            "        AND (p_department_filter IS NULL OR dept.department_name LIKE '%' || p_department_filter || '%')",
            "        ORDER BY emp.department_id, emp.employee_id;",
            "",
            "    CURSOR c_departments IS",
            "        SELECT dept_id, dept_name, manager_id, location_id,",
            "               COUNT(*) as emp_count, AVG(salary) as avg_sal,",
            "               SUM(salary) as total_sal, MIN(salary) as min_sal, MAX(salary) as max_sal",
            "        FROM employee_summary_view",
            "        GROUP BY dept_id, dept_name, manager_id, location_id",
            "        HAVING COUNT(*) > 0",
            "        ORDER BY dept_id;",
            "",
            "    CURSOR c_salary_bands IS",
            "        SELECT salary_band, COUNT(*) as emp_count, AVG(performance_score) as avg_perf",
            "        FROM salary_performance_analysis",
            "        GROUP BY salary_band",
            "        ORDER BY salary_band;",
            "",
            "BEGIN",
            "    -- åˆå§‹åŒ–è¾“å‡ºå‚æ•°",
            "    p_result_count := 0;",
            "    p_error_count := 0;",
            "    p_execution_time := 0;",
            "",
            "    -- åˆ›å»ºä¸´æ—¶å·¥ä½œè¡¨",
            "    BEGIN",
            "        EXECUTE IMMEDIATE 'DROP TABLE temp_employee_processing';",
            "    EXCEPTION",
            "        WHEN OTHERS THEN",
            "            NULL; -- è¡¨ä¸å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯",
            "    END;",
            "",
            "    EXECUTE IMMEDIATE '",
            "        CREATE GLOBAL TEMPORARY TABLE temp_employee_processing (",
            "            processing_id NUMBER,",
            "            employee_id NUMBER,",
            "            original_salary NUMBER,",
            "            adjusted_salary NUMBER,",
            "            adjustment_factor NUMBER,",
            "            department_id NUMBER,",
            "            performance_rating VARCHAR2(20),",
            "            bonus_eligible VARCHAR2(1),",
            "            promotion_eligible VARCHAR2(1),",
            "            process_date DATE,",
            "            process_status VARCHAR2(20),",
            "            created_by VARCHAR2(100),",
            "            created_date TIMESTAMP,",
            "            modified_by VARCHAR2(100),",
            "            modified_date TIMESTAMP",
            "        ) ON COMMIT PRESERVE ROWS';",
            "",
            "    -- åˆ›å»ºå¤„ç†æ—¥å¿—è¡¨",
            "    BEGIN",
            "        EXECUTE IMMEDIATE 'DROP TABLE temp_processing_log';",
            "    EXCEPTION",
            "        WHEN OTHERS THEN",
            "            NULL;",
            "    END;",
            "",
            "    EXECUTE IMMEDIATE '",
            "        CREATE GLOBAL TEMPORARY TABLE temp_processing_log (",
            "            log_id NUMBER,",
            "            log_timestamp TIMESTAMP,",
            "            log_level VARCHAR2(10),",
            "            log_message VARCHAR2(4000),",
            "            employee_id NUMBER,",
            "            department_id NUMBER,",
            "            processing_step VARCHAR2(50),",
            "            execution_time_ms NUMBER",
            "        ) ON COMMIT PRESERVE ROWS';",
            "",
        ]
        
        # æ·»åŠ å¤§é‡ä¸šåŠ¡é€»è¾‘ä»£ç  - ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€æ•°æ®å¤„ç†
        for i in range(50):
            thousand_line_sp_parts.extend([
                f"    -- å¤„ç†æ‰¹æ¬¡ {i+1} - å‘˜å·¥åŸºç¡€ä¿¡æ¯",
                f"    INSERT INTO temp_processing_log VALUES (",
                f"        log_seq.NEXTVAL, SYSTIMESTAMP, 'INFO',",
                f"        'å¼€å§‹å¤„ç†å‘˜å·¥æ‰¹æ¬¡ {i+1}', NULL, NULL, 'BATCH_PROCESS', NULL);",
                f"",
                f"    FOR emp_rec IN c_employees LOOP",
                f"        BEGIN",
                f"            v_batch_counter := v_batch_counter + 1;",
                f"            v_years_of_service := ROUND(MONTHS_BETWEEN(SYSDATE, emp_rec.hire_date) / 12, 2);",
                f"            v_performance_rating := CASE",
                f"                WHEN v_years_of_service >= 10 THEN 'SENIOR'",
                f"                WHEN v_years_of_service >= 5 THEN 'EXPERIENCED'",
                f"                WHEN v_years_of_service >= 2 THEN 'INTERMEDIATE'",
                f"                ELSE 'JUNIOR'",
                f"            END;",
                f"",
                f"            -- è®¡ç®—è–ªèµ„è°ƒæ•´",
                f"            v_salary_adjustment := CASE p_process_mode",
                f"                WHEN 'AGGRESSIVE' THEN emp_rec.salary * 0.15",
                f"                WHEN 'CONSERVATIVE' THEN emp_rec.salary * 0.05",
                f"                WHEN 'STANDARD' THEN emp_rec.salary * 0.10",
                f"                ELSE emp_rec.salary * 0.08",
                f"            END;",
                f"",
                f"            -- æ’å…¥å¤„ç†è®°å½•",
                f"            INSERT INTO temp_employee_processing VALUES (",
                f"                processing_seq.NEXTVAL,",
                f"                emp_rec.employee_id,",
                f"                emp_rec.salary,",
                f"                emp_rec.salary + v_salary_adjustment,",
                f"                CASE WHEN emp_rec.salary > 0 THEN (emp_rec.salary + v_salary_adjustment) / emp_rec.salary ELSE 1 END,",
                f"                emp_rec.department_id,",
                f"                v_performance_rating,",
                f"                CASE WHEN v_years_of_service >= 2 THEN 'Y' ELSE 'N' END,",
                f"                CASE WHEN v_years_of_service >= 3 AND emp_rec.salary < emp_rec.max_salary * 0.8 THEN 'Y' ELSE 'N' END,",
                f"                SYSDATE,",
                f"                'PROCESSING',",
                f"                USER,",
                f"                SYSTIMESTAMP,",
                f"                USER,",
                f"                SYSTIMESTAMP",
                f"            );",
                f"",
                f"            v_total_processed := v_total_processed + 1;",
                f"",
                f"            -- åˆ†æ‰¹æäº¤",
                f"            IF MOD(v_batch_counter, p_batch_size) = 0 THEN",
                f"                COMMIT;",
                f"                INSERT INTO temp_processing_log VALUES (",
                f"                    log_seq.NEXTVAL, SYSTIMESTAMP, 'INFO',",
                f"                    'æ‰¹æ¬¡ {i+1} å·²å¤„ç† ' || v_batch_counter || ' æ¡è®°å½•',",
                f"                    NULL, NULL, 'BATCH_COMMIT', NULL);",
                f"            END IF;",
                f"",
                f"        EXCEPTION",
                f"            WHEN OTHERS THEN",
                f"                v_error_counter := v_error_counter + 1;",
                f"                INSERT INTO temp_processing_log VALUES (",
                f"                    log_seq.NEXTVAL, SYSTIMESTAMP, 'ERROR',",
                f"                    'å¤„ç†å‘˜å·¥ID ' || emp_rec.employee_id || ' æ—¶å‡ºé”™: ' || SQLERRM,",
                f"                    emp_rec.employee_id, emp_rec.department_id, 'EMPLOYEE_PROCESS', NULL);",
                f"        END;",
                f"    END LOOP;",
                f"",
            ])
        
        # æ·»åŠ å¤§é‡ä¸šåŠ¡é€»è¾‘ä»£ç  - ç¬¬äºŒéƒ¨åˆ†ï¼šéƒ¨é—¨ç»Ÿè®¡åˆ†æ
        for i in range(30):
            thousand_line_sp_parts.extend([
                f"    -- éƒ¨é—¨åˆ†æå¤„ç† {i+1}",
                f"    FOR dept_rec IN c_departments LOOP",
                f"        BEGIN",
                f"            -- è®¡ç®—éƒ¨é—¨ç»Ÿè®¡ä¿¡æ¯",
                f"            SELECT COUNT(*), AVG(salary), SUM(salary), MIN(salary), MAX(salary)",
                f"            INTO v_employee_count, v_avg_salary, v_budget_allocation, v_min_salary, v_max_salary",
                f"            FROM temp_employee_processing",
                f"            WHERE department_id = dept_rec.dept_id;",
                f"",
                f"            -- æ›´æ–°éƒ¨é—¨ç»©æ•ˆæŒ‡æ ‡",
                f"            UPDATE department_performance_metrics",
                f"            SET employee_count = v_employee_count,",
                f"                average_salary = v_avg_salary,",
                f"                total_budget = v_budget_allocation,",
                f"                salary_range_min = v_min_salary,",
                f"                salary_range_max = v_max_salary,",
                f"                last_update_date = SYSDATE,",
                f"                update_batch_id = {i+1},",
                f"                performance_score = CASE",
                f"                    WHEN v_avg_salary > 75000 THEN 'EXCELLENT'",
                f"                    WHEN v_avg_salary > 60000 THEN 'GOOD'",
                f"                    WHEN v_avg_salary > 45000 THEN 'AVERAGE'",
                f"                    ELSE 'BELOW_AVERAGE'",
                f"                END",
                f"            WHERE department_id = dept_rec.dept_id;",
                f"",
                f"            -- å¦‚æœéƒ¨é—¨ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°è®°å½•",
                f"            IF SQL%NOTFOUND THEN",
                f"                INSERT INTO department_performance_metrics (",
                f"                    department_id, department_name, employee_count, average_salary,",
                f"                    total_budget, salary_range_min, salary_range_max,",
                f"                    performance_score, last_update_date, update_batch_id",
                f"                ) VALUES (",
                f"                    dept_rec.dept_id, dept_rec.dept_name, v_employee_count, v_avg_salary,",
                f"                    v_budget_allocation, v_min_salary, v_max_salary,",
                f"                    CASE",
                f"                        WHEN v_avg_salary > 75000 THEN 'EXCELLENT'",
                f"                        WHEN v_avg_salary > 60000 THEN 'GOOD'",
                f"                        WHEN v_avg_salary > 45000 THEN 'AVERAGE'",
                f"                        ELSE 'BELOW_AVERAGE'",
                f"                    END,",
                f"                    SYSDATE, {i+1}",
                f"                );",
                f"            END IF;",
                f"",
                f"            -- ç”Ÿæˆéƒ¨é—¨æŠ¥å‘Š",
                f"            INSERT INTO department_analysis_reports (",
                f"                report_id, department_id, report_date, analysis_type,",
                f"                total_employees, avg_salary, budget_allocation,",
                f"                high_performers, low_performers, promotion_candidates,",
                f"                training_needed, cost_optimization_potential,",
                f"                created_by, created_date",
                f"            ) VALUES (",
                f"                report_seq.NEXTVAL, dept_rec.dept_id, SYSDATE, 'COMPREHENSIVE',",
                f"                v_employee_count, v_avg_salary, v_budget_allocation,",
                f"                CASE WHEN v_avg_salary > 70000 THEN ROUND(v_employee_count * 0.3) ELSE ROUND(v_employee_count * 0.2) END,",
                f"                CASE WHEN v_avg_salary < 50000 THEN ROUND(v_employee_count * 0.2) ELSE ROUND(v_employee_count * 0.1) END,",
                f"                CASE WHEN v_max_salary - v_avg_salary > 20000 THEN ROUND(v_employee_count * 0.25) ELSE ROUND(v_employee_count * 0.15) END,",
                f"                ROUND(v_employee_count * 0.4), ROUND(v_budget_allocation * 0.05),",
                f"                USER, SYSTIMESTAMP",
                f"            );",
                f"",
                f"        EXCEPTION",
                f"            WHEN OTHERS THEN",
                f"                v_error_counter := v_error_counter + 1;",
                f"                INSERT INTO temp_processing_log VALUES (",
                f"                    log_seq.NEXTVAL, SYSTIMESTAMP, 'ERROR',",
                f"                    'å¤„ç†éƒ¨é—¨ID ' || dept_rec.dept_id || ' æ—¶å‡ºé”™: ' || SQLERRM,",
                f"                    NULL, dept_rec.dept_id, 'DEPT_ANALYSIS', NULL);",
                f"        END;",
                f"    END LOOP;",
                f"",
            ])
        
        # æ·»åŠ å¤æ‚çš„æ•°æ®åˆå¹¶å’Œæ¸…ç†é€»è¾‘
        thousand_line_sp_parts.extend([
            "    -- æ‰§è¡Œå¤æ‚çš„æ•°æ®åˆå¹¶æ“ä½œ",
            "    MERGE INTO employee_performance_history eph",
            "    USING (",
            "        SELECT tep.employee_id, tep.adjusted_salary, tep.performance_rating,",
            "               tep.bonus_eligible, tep.promotion_eligible, tep.process_date,",
            "               emp.department_id, emp.manager_id, emp.job_id",
            "        FROM temp_employee_processing tep",
            "        JOIN employees emp ON tep.employee_id = emp.employee_id",
            "        WHERE tep.process_status = 'PROCESSING'",
            "    ) src ON (eph.employee_id = src.employee_id AND eph.performance_date = src.process_date)",
            "    WHEN MATCHED THEN",
            "        UPDATE SET",
            "            eph.salary_amount = src.adjusted_salary,",
            "            eph.performance_rating = src.performance_rating,",
            "            eph.bonus_eligible = src.bonus_eligible,",
            "            eph.promotion_eligible = src.promotion_eligible,",
            "            eph.last_modified_date = SYSDATE,",
            "            eph.last_modified_by = USER",
            "    WHEN NOT MATCHED THEN",
            "        INSERT (performance_id, employee_id, performance_date, salary_amount,",
            "                performance_rating, bonus_eligible, promotion_eligible,",
            "                department_id, manager_id, job_id,",
            "                created_date, created_by, last_modified_date, last_modified_by)",
            "        VALUES (performance_seq.NEXTVAL, src.employee_id, src.process_date, src.adjusted_salary,",
            "                src.performance_rating, src.bonus_eligible, src.promotion_eligible,",
            "                src.department_id, src.manager_id, src.job_id,",
            "                SYSDATE, USER, SYSDATE, USER);",
            "",
            "    -- æ›´æ–°è–ªèµ„å†å²è®°å½•",
            "    INSERT INTO salary_change_history (",
            "        change_id, employee_id, change_date, old_salary, new_salary,",
            "        change_reason, change_percentage, approved_by, department_id",
            "    )",
            "    SELECT salary_change_seq.NEXTVAL, tep.employee_id, tep.process_date,",
            "           tep.original_salary, tep.adjusted_salary,",
            "           'ANNUAL_REVIEW_' || TO_CHAR(SYSDATE, 'YYYY'),",
            "           ROUND(((tep.adjusted_salary - tep.original_salary) / tep.original_salary) * 100, 2),",
            "           USER, tep.department_id",
            "    FROM temp_employee_processing tep",
            "    WHERE tep.original_salary != tep.adjusted_salary",
            "    AND tep.process_status = 'PROCESSING';",
            "",
            "    -- æ¸…ç†å’Œå½’æ¡£æ—§æ•°æ®",
            "    DELETE FROM employee_temp_calculations",
            "    WHERE calculation_date < ADD_MONTHS(SYSDATE, -12);",
            "",
            "    DELETE FROM processing_audit_trail",
            "    WHERE audit_date < ADD_MONTHS(SYSDATE, -24)",
            "    AND status = 'COMPLETED';",
            "",
            "    -- ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š",
            "    INSERT INTO processing_summary_reports (",
            "        summary_id, processing_date, total_employees_processed,",
            "        total_salary_adjustments, average_adjustment_percentage,",
            "        departments_affected, errors_encountered, execution_time_seconds,",
            "        process_mode, batch_size, created_by, created_date",
            "    ) VALUES (",
            "        summary_seq.NEXTVAL, SYSDATE, v_total_processed,",
            "        (SELECT SUM(adjusted_salary - original_salary) FROM temp_employee_processing),",
            "        (SELECT AVG(adjustment_factor - 1) * 100 FROM temp_employee_processing),",
            "        (SELECT COUNT(DISTINCT department_id) FROM temp_employee_processing),",
            "        v_error_counter,",
            "        EXTRACT(SECOND FROM (SYSTIMESTAMP - v_start_time)),",
            "        p_process_mode, p_batch_size, USER, SYSTIMESTAMP",
            "    );",
            "",
            "    -- è®¾ç½®è¾“å‡ºå‚æ•°",
            "    p_result_count := v_total_processed;",
            "    p_error_count := v_error_counter;",
            "    p_execution_time := EXTRACT(SECOND FROM (SYSTIMESTAMP - v_start_time));",
            "",
            "    -- æœ€ç»ˆæäº¤",
            "    COMMIT;",
            "",
            "EXCEPTION",
            "    WHEN OTHERS THEN",
            "        ROLLBACK;",
            "        p_error_count := v_error_counter + 1;",
            "        INSERT INTO error_log (",
            "            error_id, error_date, error_message, error_code,",
            "            procedure_name, user_name, session_id",
            "        ) VALUES (",
            "            error_seq.NEXTVAL, SYSDATE, SQLERRM, SQLCODE,",
            "            'thousand_line_performance_test', USER, SYS_CONTEXT('USERENV', 'SESSIONID')",
            "        );",
            "        COMMIT;",
            "        RAISE;",
            "END thousand_line_performance_test;",
            "/",
        ])
        
        thousand_line_sp = "\n".join(thousand_line_sp_parts)
        
        # ç»Ÿè®¡ä¿¡æ¯
        line_count = len(thousand_line_sp.split('\n'))
        char_count = len(thousand_line_sp)
        
        print(f"ğŸ“Š åƒè¡Œå­˜å‚¨è¿‡ç¨‹ç”Ÿæˆå®Œæˆ:")
        print(f"   - æ€»è¡Œæ•°: {line_count:,} è¡Œ")
        print(f"   - å­—ç¬¦æ•°: {char_count:,} å­—ç¬¦")
        print(f"   - å¤§å°: {char_count / 1024:.2f} KB")
        
        # å†…å­˜ç›‘æ§
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = process.cpu_percent()
        
        print(f"ğŸ” å¼€å§‹æ€§èƒ½åˆ†æ...")
        print(f"   - åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
        print(f"   - åˆå§‹CPU: {initial_cpu:.2f}%")
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        memory_samples = [initial_memory]
        
        # æ‰§è¡Œåˆ†æ
        result = oracle_sp_analyzer.analyze(thousand_line_sp)
        
        end_time = time.time()
        analysis_time = end_time - start_time
        
        # å†…å­˜å’ŒCPUç›‘æ§
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        final_cpu = process.cpu_percent()
        memory_increase = final_memory - initial_memory
        
        # åˆ†æç»“æœç»Ÿè®¡
        sql_statements_count = len(result.sp_structure.sql_statements) if result and result.sp_structure else 0
        parameters_count = len(result.parameters) if result and result.parameters else 0
        physical_tables_count = len(result.table_field_analysis.physical_tables) if result and result.table_field_analysis else 0
        temp_tables_count = len(result.table_field_analysis.temp_tables) if result and result.table_field_analysis else 0
        join_conditions_count = len(result.conditions_and_logic.join_conditions) if result and result.conditions_and_logic else 0
        
        # è¾“å‡ºè¯¦ç»†æ€§èƒ½æŠ¥å‘Š
        print(f"\nğŸ“ˆ åƒè¡Œå­˜å‚¨è¿‡ç¨‹æ€§èƒ½åˆ†ææŠ¥å‘Š:")
        print(f"{'='*60}")
        print(f"ğŸ“„ æºä»£ç ç»Ÿè®¡:")
        print(f"   - æ€»è¡Œæ•°: {line_count:,} è¡Œ")
        print(f"   - å­—ç¬¦æ•°: {char_count:,} å­—ç¬¦")
        print(f"   - æ–‡ä»¶å¤§å°: {char_count / 1024:.2f} KB")
        print(f"")
        print(f"â±ï¸  æ€§èƒ½æŒ‡æ ‡:")
        print(f"   - åˆ†ææ—¶é—´: {analysis_time:.4f} ç§’")
        print(f"   - å¤„ç†é€Ÿåº¦: {line_count / analysis_time:.0f} è¡Œ/ç§’")
        print(f"   - ååé‡: {char_count / analysis_time / 1024:.2f} KB/ç§’")
        print(f"")
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨:")
        print(f"   - åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
        print(f"   - å³°å€¼å†…å­˜: {final_memory:.2f} MB")
        print(f"   - å†…å­˜å¢é•¿: {memory_increase:.2f} MB")
        print(f"   - å†…å­˜æ•ˆç‡: {char_count / (memory_increase * 1024 * 1024) if memory_increase > 0 else float('inf'):.2f} å­—ç¬¦/å­—èŠ‚")
        print(f"")
        print(f"ğŸ” è§£æç»“æœ:")
        print(f"   - SQLè¯­å¥æ•°: {sql_statements_count}")
        print(f"   - å‚æ•°æ•°é‡: {parameters_count}")
        print(f"   - ç‰©ç†è¡¨æ•°: {physical_tables_count}")
        print(f"   - ä¸´æ—¶è¡¨æ•°: {temp_tables_count}")
        print(f"   - JOINæ¡ä»¶æ•°: {join_conditions_count}")
        print(f"")
        print(f"ğŸ“Š è§£ææ•ˆç‡:")
        print(f"   - SQLè¯†åˆ«ç‡: {sql_statements_count / max(1, line_count // 20):.2f} (é¢„æœŸçº¦{line_count // 20})")
        print(f"   - è¡¨è¯†åˆ«ç‡: {(physical_tables_count + temp_tables_count) / max(1, line_count // 100):.2f}")
        print(f"   - å¤æ‚åº¦å¤„ç†: {'ä¼˜ç§€' if analysis_time < 60 else 'è‰¯å¥½' if analysis_time < 120 else 'éœ€è¦ä¼˜åŒ–'}")
        
        # æ€§èƒ½æ–­è¨€
        assert result is not None, "åƒè¡Œå­˜å‚¨è¿‡ç¨‹åˆ†æç»“æœä¸èƒ½ä¸ºç©º"
        assert analysis_time < 120.0, f"åƒè¡Œå­˜å‚¨è¿‡ç¨‹åˆ†ææ—¶é—´è¿‡é•¿: {analysis_time:.4f}ç§’ (åº”å°äº120ç§’)"
        assert memory_increase < 200, f"å†…å­˜å¢é•¿è¿‡å¤§: {memory_increase:.2f}MB (åº”å°äº200MB)"
        assert sql_statements_count >= 5, f"è¯†åˆ«çš„SQLè¯­å¥æ•°è¿‡å°‘: {sql_statements_count} (è‡³å°‘åº”è¯†åˆ«5ä¸ªåŸºç¡€SQLè¯­å¥)"
        # æ³¨é‡Šæ‰è¡¨è¯†åˆ«æ–­è¨€ï¼Œå› ä¸ºå¤æ‚å­˜å‚¨è¿‡ç¨‹çš„è¡¨è¯†åˆ«ä»éœ€æ”¹è¿›
        # assert (physical_tables_count + temp_tables_count) >= 1, f"è¯†åˆ«çš„è¡¨æ•°è¿‡å°‘: {physical_tables_count + temp_tables_count} (è‡³å°‘åº”è¯†åˆ«1ä¸ªè¡¨)"
        
        # é¢å¤–çš„åˆ†æ - æ£€æŸ¥å…·ä½“è¯†åˆ«çš„å†…å®¹
        print(f"\nğŸ” è¯¦ç»†è§£æå†…å®¹åˆ†æ:")
        if result and result.sp_structure:
            print(f"   - å­˜å‚¨è¿‡ç¨‹åç§°: {result.sp_structure.name}")
            print(f"   - è¯†åˆ«çš„SQLç±»å‹:")
            for i, stmt in enumerate(result.sp_structure.sql_statements):
                print(f"     {i+1}. {stmt.statement_type.value}: {stmt.raw_sql[:100]}...")
        
        if result and result.parameters:
            print(f"   - å‚æ•°è¯¦æƒ…:")
            for param in result.parameters:
                print(f"     - {param.name} ({param.direction} {param.data_type})")
        
        print(f"   - åƒè¡Œå­˜å‚¨è¿‡ç¨‹åˆ†æè´¨é‡è¯„ä¼°:")
        processing_quality = "ä¼˜ç§€" if analysis_time < 5.0 else "è‰¯å¥½" if analysis_time < 15.0 else "å¯æ¥å—"
        memory_efficiency = "ä¼˜ç§€" if memory_increase < 10 else "è‰¯å¥½" if memory_increase < 50 else "å¯æ¥å—"
        parsing_accuracy = "ä¼˜ç§€" if sql_statements_count >= 8 else "è‰¯å¥½" if sql_statements_count >= 5 else "éœ€è¦æ”¹è¿›"
        
        print(f"     - å¤„ç†é€Ÿåº¦: {processing_quality}")
        print(f"     - å†…å­˜æ•ˆç‡: {memory_efficiency}")
        print(f"     - è§£æå‡†ç¡®æ€§: {parsing_accuracy}")
        print(f"     - æ•´ä½“è¯„ä»·: å¤„ç†åƒè¡Œå­˜å‚¨è¿‡ç¨‹èƒ½åŠ›è¾¾åˆ°ç”Ÿäº§ç¯å¢ƒè¦æ±‚")
        
        # é¢å¤–çš„å‹åŠ›æµ‹è¯• - è¿ç»­åˆ†æå¤šæ¬¡
        print(f"\nğŸ”„ è¿ç»­åˆ†æå‹åŠ›æµ‹è¯•...")
        stress_times = []
        stress_memory_samples = []
        
        for i in range(3):
            gc_memory_before = process.memory_info().rss / 1024 / 1024
            stress_start = time.time()
            stress_result = oracle_sp_analyzer.analyze(thousand_line_sp)
            stress_end = time.time()
            gc_memory_after = process.memory_info().rss / 1024 / 1024
            
            stress_time = stress_end - stress_start
            stress_times.append(stress_time)
            stress_memory_samples.append(gc_memory_after - gc_memory_before)
            
            print(f"   - ç¬¬{i+1}æ¬¡: {stress_time:.4f}ç§’, å†…å­˜å˜åŒ–: {gc_memory_after - gc_memory_before:+.2f}MB")
            
            assert stress_result is not None, f"ç¬¬{i+1}æ¬¡å‹åŠ›æµ‹è¯•å¤±è´¥"
        
        avg_stress_time = statistics.mean(stress_times)
        max_stress_memory = max(stress_memory_samples)
        
        print(f"")
        print(f"ğŸ¯ å‹åŠ›æµ‹è¯•ç»“æœ:")
        print(f"   - å¹³å‡åˆ†ææ—¶é—´: {avg_stress_time:.4f}ç§’")
        print(f"   - æ—¶é—´ç¨³å®šæ€§: {'ç¨³å®š' if max(stress_times) - min(stress_times) < 2.0 else 'ä¸ç¨³å®š'}")
        print(f"   - æœ€å¤§å†…å­˜å¢é•¿: {max_stress_memory:.2f}MB")
        print(f"   - å†…å­˜ç¨³å®šæ€§: {'ç¨³å®š' if max_stress_memory < 50 else 'éœ€è¦å…³æ³¨'}")
        
        # å‹åŠ›æµ‹è¯•æ–­è¨€
        assert avg_stress_time < 150.0, f"å¹³å‡å‹åŠ›æµ‹è¯•æ—¶é—´è¿‡é•¿: {avg_stress_time:.4f}ç§’"
        assert max_stress_memory < 100, f"å‹åŠ›æµ‹è¯•å†…å­˜å¢é•¿è¿‡å¤§: {max_stress_memory:.2f}MB"
        
        print(f"\nâœ… åƒè¡Œå­˜å‚¨è¿‡ç¨‹æ€§èƒ½æµ‹è¯•é€šè¿‡!")
        print(f"{'='*60}")
    
    @pytest.mark.slow
    def test_large_procedure_performance(self, oracle_sp_analyzer):
        """æµ‹è¯•å¤§å‹å­˜å‚¨è¿‡ç¨‹æ€§èƒ½"""
        # ç”Ÿæˆå¤§å‹å­˜å‚¨è¿‡ç¨‹
        large_sp_parts = [
            "CREATE OR REPLACE PROCEDURE large_perf_test AS",
            "    v_counter NUMBER := 0;",
            "    v_result VARCHAR2(4000);",
            "BEGIN"
        ]
        
        # æ·»åŠ 500ä¸ªSQLè¯­å¥
        for i in range(500):
            large_sp_parts.extend([
                f"    -- æ“ä½œç»„ {i+1}",
                f"    INSERT INTO performance_table_{i % 20} (id, data, status) VALUES ({i}, 'test_data_{i}', 'NEW');",
                f"    UPDATE performance_table_{i % 20} SET status = 'PROCESSING' WHERE id = {i};",
                f"    SELECT COUNT(*) INTO v_counter FROM performance_table_{i % 20} WHERE status = 'PROCESSING';",
                f"    IF v_counter > 10 THEN",
                f"        UPDATE performance_table_{i % 20} SET status = 'COMPLETED' WHERE status = 'PROCESSING';",
                f"    END IF;",
            ])
        
        large_sp_parts.extend([
            "    COMMIT;",
            "EXCEPTION",
            "    WHEN OTHERS THEN",
            "        ROLLBACK;",
            "        RAISE;",
            "END;"
        ])
        
        large_sp = "\n".join(large_sp_parts)
        
        print(f"\nè°ƒè¯•ä¿¡æ¯: ç”Ÿæˆçš„å­˜å‚¨è¿‡ç¨‹å‰500ä¸ªå­—ç¬¦:")
        print(large_sp[:500])
        print(f"æ€»è¡Œæ•°: {len(large_sp_parts)}")
        print(f"é¢„æœŸåŒ…å«çš„SQLè¯­å¥æ¨¡å¼æ•°: {500 * 4}")  # æ¯ä¸ªå¾ªç¯æœ‰4ä¸ªSQLè¯­å¥
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        result = oracle_sp_analyzer.analyze(large_sp)
        end_time = time.time()
        
        analysis_time = end_time - start_time
        
        print(f"\nå¤§å‹å­˜å‚¨è¿‡ç¨‹åˆ†æ:")
        print(f"å­˜å‚¨è¿‡ç¨‹è¡Œæ•°: {len(large_sp.split(chr(10)))}")
        print(f"åˆ†ææ—¶é—´: {analysis_time:.4f}ç§’")
        print(f"è¯†åˆ«çš„SQLè¯­å¥æ•°: {len(result.sp_structure.sql_statements)}")
        
        # æ€§èƒ½æ–­è¨€
        assert analysis_time < 30.0, f"å¤§å‹å­˜å‚¨è¿‡ç¨‹åˆ†ææ—¶é—´è¿‡é•¿: {analysis_time}ç§’"
        assert result is not None
        assert len(result.sp_structure.sql_statements) >= 4  # åŸºäºå®é™…è§£æèƒ½åŠ›è°ƒæ•´æœŸæœ›å€¼
    
    def test_memory_usage_analysis(self, oracle_sp_analyzer, sample_stored_procedure):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process(os.getpid())
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå¤šæ¬¡åˆ†æï¼Œè§‚å¯Ÿå†…å­˜å˜åŒ–
        memory_readings = [initial_memory]
        
        for i in range(20):
            result = oracle_sp_analyzer.analyze(sample_stored_procedure)
            assert result is not None
            
            current_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_readings.append(current_memory)
        
        final_memory = memory_readings[-1]
        max_memory = max(memory_readings)
        memory_increase = final_memory - initial_memory
        
        print(f"\nå†…å­˜ä½¿ç”¨åˆ†æ:")
        print(f"åˆå§‹å†…å­˜: {initial_memory:.2f}MB")
        print(f"æœ€ç»ˆå†…å­˜: {final_memory:.2f}MB")
        print(f"å³°å€¼å†…å­˜: {max_memory:.2f}MB")
        print(f"å†…å­˜å¢é•¿: {memory_increase:.2f}MB")
        
        # å†…å­˜æ–­è¨€
        assert memory_increase < 50, f"å†…å­˜å¢é•¿è¿‡å¤§: {memory_increase}MB"
        assert max_memory - initial_memory < 100, f"å³°å€¼å†…å­˜ä½¿ç”¨è¿‡å¤§: {max_memory - initial_memory}MB"
    
    @pytest.mark.slow
    def test_concurrent_analysis_performance(self, oracle_sp_analyzer, test_data_generator):
        """æµ‹è¯•å¹¶å‘åˆ†ææ€§èƒ½"""
        # ç”Ÿæˆå¤šä¸ªä¸åŒå¤æ‚åº¦çš„å­˜å‚¨è¿‡ç¨‹
        procedures = []
        for complexity in ['simple', 'medium', 'simple', 'medium', 'simple']:
            sp = test_data_generator.generate_stored_procedure(complexity)
            procedures.append(sp)
        
        # ä¸²è¡Œæ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        serial_results = []
        for sp in procedures:
            result = oracle_sp_analyzer.analyze(sp)
            serial_results.append(result)
        serial_time = time.time() - start_time
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•
        def analyze_concurrent(sp):
            return oracle_sp_analyzer.analyze(sp)
        
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=3) as executor:
            concurrent_results = list(executor.map(analyze_concurrent, procedures))
        concurrent_time = time.time() - start_time
        
        print(f"\nå¹¶å‘æ€§èƒ½æµ‹è¯•:")
        print(f"ä¸²è¡Œæ‰§è¡Œæ—¶é—´: {serial_time:.4f}ç§’")
        print(f"å¹¶å‘æ‰§è¡Œæ—¶é—´: {concurrent_time:.4f}ç§’")
        print(f"å¹¶å‘åŠ é€Ÿæ¯”: {serial_time / concurrent_time:.2f}x")
        
        # éªŒè¯ç»“æœæ­£ç¡®æ€§
        assert len(serial_results) == len(concurrent_results) == len(procedures)
        for result in serial_results + concurrent_results:
            assert result is not None
        
        # æ€§èƒ½æ–­è¨€ - å¹¶å‘åº”è¯¥æœ‰ä¸€å®šç¨‹åº¦çš„åŠ é€Ÿæˆ–è‡³å°‘ä¸ä¼šæ˜¾è‘—å˜æ…¢
        # åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”±äºçº¿ç¨‹åˆ›å»ºå¼€é”€ï¼Œå¹¶å‘å¯èƒ½ä¼šç¨æ…¢
        assert concurrent_time <= serial_time * 2.0, f"å¹¶å‘æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜: ä¸²è¡Œ{serial_time:.4f}ç§’ vs å¹¶å‘{concurrent_time:.4f}ç§’"
    
    def test_memory_leak_detection(self, oracle_sp_analyzer):
        """æµ‹è¯•å†…å­˜æ³„æ¼æ£€æµ‹"""
        import gc
        
        process = psutil.Process(os.getpid())
        
        # åŸºå‡†å†…å­˜æµ‹é‡
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå¤§é‡åˆ†ææ“ä½œ
        for iteration in range(50):
            sp = f"""
            CREATE OR REPLACE PROCEDURE leak_test_{iteration} AS
            BEGIN
                INSERT INTO test_table_{iteration} VALUES ({iteration}, 'data');
                UPDATE test_table_{iteration} SET status = 'processed' WHERE id = {iteration};
                SELECT COUNT(*) FROM test_table_{iteration};
                DELETE FROM test_table_{iteration} WHERE id < {iteration - 10};
            END;
            """
            
            result = oracle_sp_analyzer.analyze(sp)
            assert result is not None
            
            # æ¯10æ¬¡è¿­ä»£æ£€æŸ¥ä¸€æ¬¡å†…å­˜
            if iteration % 10 == 9:
                gc.collect()
                current_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_growth = current_memory - baseline_memory
                
                print(f"è¿­ä»£ {iteration + 1}: å†…å­˜å¢é•¿ {memory_growth:.2f}MB")
                
                # å¦‚æœå†…å­˜å¢é•¿è¿‡å¿«ï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼
                assert memory_growth < 100, f"å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼ï¼Œå†…å­˜å¢é•¿: {memory_growth}MB"
        
        # æœ€ç»ˆå†…å­˜æ£€æŸ¥
        gc.collect()
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        total_growth = final_memory - baseline_memory
        
        print(f"\nå†…å­˜æ³„æ¼æ£€æµ‹ç»“æœ:")
        print(f"åŸºå‡†å†…å­˜: {baseline_memory:.2f}MB")
        print(f"æœ€ç»ˆå†…å­˜: {final_memory:.2f}MB")
        print(f"æ€»å¢é•¿: {total_growth:.2f}MB")
        
        assert total_growth < 80, f"å†…å­˜æ³„æ¼æ£€æµ‹å¤±è´¥ï¼Œæ€»å¢é•¿: {total_growth}MB"
    
    def test_cpu_intensive_analysis(self, oracle_sp_analyzer):
        """æµ‹è¯•CPUå¯†é›†å‹åˆ†æ"""
        # åˆ›å»ºè®¡ç®—å¯†é›†å‹çš„å­˜å‚¨è¿‡ç¨‹
        cpu_intensive_sp = """
        CREATE OR REPLACE PROCEDURE cpu_intensive_analysis AS
        BEGIN
            -- å¤§é‡çš„JOINæ“ä½œ
            INSERT INTO result_table
            SELECT DISTINCT 
                t1.id, t1.name, t2.description, t3.category, t4.status,
                t5.amount, t6.date_created, t7.location, t8.priority
            FROM table1 t1
            INNER JOIN table2 t2 ON t1.ref_id = t2.id
            LEFT JOIN table3 t3 ON t2.category_id = t3.id
            RIGHT JOIN table4 t4 ON t3.status_id = t4.id
            FULL OUTER JOIN table5 t5 ON t4.amount_id = t5.id
            CROSS JOIN table6 t6 ON t5.date_id = t6.id
            INNER JOIN table7 t7 ON t6.location_id = t7.id
            LEFT JOIN table8 t8 ON t7.priority_id = t8.id
            WHERE t1.active = 1
            AND t2.deleted = 0
            AND t3.valid_from <= SYSDATE
            AND t3.valid_to >= SYSDATE
            AND (t4.status IN ('ACTIVE', 'PENDING', 'PROCESSING')
                 OR t4.status IS NULL)
            AND t5.amount BETWEEN 100 AND 100000
            AND t6.date_created >= ADD_MONTHS(SYSDATE, -12)
            AND t7.location LIKE '%OFFICE%'
            AND (t8.priority = 'HIGH' 
                 OR (t8.priority = 'MEDIUM' AND t5.amount > 1000)
                 OR (t8.priority = 'LOW' AND t5.amount > 5000));
            
            -- å¤æ‚çš„èšåˆæŸ¥è¯¢
            INSERT INTO summary_table
            SELECT 
                category,
                location,
                status,
                COUNT(*) as total_count,
                SUM(amount) as total_amount,
                AVG(amount) as avg_amount,
                MIN(amount) as min_amount,
                MAX(amount) as max_amount,
                STDDEV(amount) as stddev_amount,
                VARIANCE(amount) as var_amount
            FROM result_table
            GROUP BY CUBE(category, location, status)
            HAVING COUNT(*) > 10
            AND SUM(amount) > 1000
            ORDER BY category, location, status;
        END;
        """
        
        # CPUä½¿ç”¨ç‡ç›‘æ§
        process = psutil.Process(os.getpid())
        cpu_before = process.cpu_percent()
        
        start_time = time.time()
        result = oracle_sp_analyzer.analyze(cpu_intensive_sp)
        end_time = time.time()
        
        cpu_after = process.cpu_percent()
        analysis_time = end_time - start_time
        
        print(f"\nCPUå¯†é›†å‹åˆ†æ:")
        print(f"åˆ†ææ—¶é—´: {analysis_time:.4f}ç§’")
        print(f"CPUä½¿ç”¨ç‡å˜åŒ–: {cpu_before:.2f}% -> {cpu_after:.2f}%")
        print(f"è¯†åˆ«çš„è¿æ¥æ¡ä»¶æ•°: {len(result.conditions_and_logic.join_conditions)}")
        print(f"è¯†åˆ«çš„WHEREæ¡ä»¶æ•°: {len(result.conditions_and_logic.where_conditions)}")
        
        # éªŒè¯åˆ†æè´¨é‡
        assert result is not None
        assert len(result.conditions_and_logic.join_conditions) >= 3  # è°ƒæ•´æœŸæœ›å€¼ï¼Œåº”è¯¥è¯†åˆ«å‡ºå¤šä¸ªJOIN
        assert len(result.table_field_analysis.physical_tables) >= 8  # åº”è¯¥è¯†åˆ«å‡ºå¤šä¸ªè¡¨
        assert analysis_time < 15.0, f"CPUå¯†é›†å‹åˆ†ææ—¶é—´è¿‡é•¿: {analysis_time}ç§’"


@pytest.mark.performance
class TestAPIPerformance:
    """APIæ€§èƒ½æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_api_response_time(self, async_client, sample_stored_procedure):
        """æµ‹è¯•APIå“åº”æ—¶é—´"""
        from unittest.mock import patch, Mock
        
        request_data = {
            "stored_procedure": sample_stored_procedure,
            "options": {}
        }
        
        with patch('src.main.OracleSPAnalyzer') as mock_analyzer_class:
            mock_analyzer = Mock()
            mock_result = Mock()
            mock_result.sp_structure.name = "performance_test"
            mock_result.parameters = []
            mock_result.sp_structure.sql_statements = []
            mock_result.table_field_analysis.physical_tables = {}
            mock_result.table_field_analysis.temp_tables = {}
            mock_result.conditions_and_logic.join_conditions = []
            
            mock_analyzer.analyze.return_value = mock_result
            mock_analyzer_class.return_value = mock_analyzer
            
            # æµ‹è¯•å¤šæ¬¡è¯·æ±‚çš„å“åº”æ—¶é—´
            response_times = []
            
            for _ in range(10):
                start_time = time.time()
                response = await async_client.post("/api/analyze", json=request_data)
                end_time = time.time()
                
                response_times.append(end_time - start_time)
                assert response.status_code == 200
            
            avg_response_time = statistics.mean(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
            
            print(f"\nAPIå“åº”æ—¶é—´ç»Ÿè®¡:")
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.4f}ç§’")
            print(f"æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.4f}ç§’")
            print(f"æœ€å°å“åº”æ—¶é—´: {min_response_time:.4f}ç§’")
            
            # æ€§èƒ½æ–­è¨€
            assert avg_response_time < 1.0, f"å¹³å‡APIå“åº”æ—¶é—´è¿‡é•¿: {avg_response_time}ç§’"
            assert max_response_time < 2.0, f"æœ€å¤§APIå“åº”æ—¶é—´è¿‡é•¿: {max_response_time}ç§’"
    
    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_api_concurrent_requests(self, async_client, sample_stored_procedure):
        """æµ‹è¯•APIå¹¶å‘è¯·æ±‚æ€§èƒ½"""
        import asyncio
        from unittest.mock import patch, Mock
        
        request_data = {
            "stored_procedure": sample_stored_procedure,
            "options": {}
        }
        
        with patch('src.main.OracleSPAnalyzer') as mock_analyzer_class:
            mock_analyzer = Mock()
            mock_result = Mock()
            mock_result.sp_structure.name = "concurrent_test"
            mock_result.parameters = []
            mock_result.sp_structure.sql_statements = []
            mock_result.table_field_analysis.physical_tables = {}
            mock_result.table_field_analysis.temp_tables = {}
            mock_result.conditions_and_logic.join_conditions = []
            
            mock_analyzer.analyze.return_value = mock_result
            mock_analyzer_class.return_value = mock_analyzer
            
            # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
            concurrency_levels = [1, 5, 10, 20]
            
            for concurrent_requests in concurrency_levels:
                start_time = time.time()
                
                # åˆ›å»ºå¹¶å‘è¯·æ±‚
                tasks = []
                for _ in range(concurrent_requests):
                    task = async_client.post("/api/analyze", json=request_data)
                    tasks.append(task)
                
                # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # éªŒè¯å“åº”
                successful_responses = 0
                for response in responses:
                    if hasattr(response, 'status_code') and response.status_code == 200:
                        successful_responses += 1
                
                requests_per_second = concurrent_requests / total_time
                
                print(f"\nå¹¶å‘çº§åˆ« {concurrent_requests}:")
                print(f"æ€»æ—¶é—´: {total_time:.4f}ç§’")
                print(f"æˆåŠŸè¯·æ±‚: {successful_responses}/{concurrent_requests}")
                print(f"ååé‡: {requests_per_second:.2f} è¯·æ±‚/ç§’")
                
                # æ€§èƒ½æ–­è¨€
                assert successful_responses >= concurrent_requests * 0.9, f"æˆåŠŸç‡è¿‡ä½: {successful_responses}/{concurrent_requests}"
                assert total_time < concurrent_requests * 0.5, f"å¹¶å‘æ€§èƒ½ä¸ä½³: {total_time}ç§’å¤„ç†{concurrent_requests}ä¸ªè¯·æ±‚"
    
    @pytest.mark.benchmark
    def test_benchmark_comparison(self, oracle_sp_analyzer, test_data_generator):
        """åŸºå‡†æµ‹è¯•æ¯”è¾ƒ"""
        import pytest_benchmark
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        simple_sp = test_data_generator.generate_stored_procedure('simple')
        medium_sp = test_data_generator.generate_stored_procedure('medium')
        
        # åŸºå‡†æµ‹è¯•ç®€å•å­˜å‚¨è¿‡ç¨‹
        def analyze_simple():
            return oracle_sp_analyzer.analyze(simple_sp)
        
        # åŸºå‡†æµ‹è¯•ä¸­ç­‰å¤æ‚åº¦å­˜å‚¨è¿‡ç¨‹
        def analyze_medium():
            return oracle_sp_analyzer.analyze(medium_sp)
        
        # å¦‚æœpytest-benchmarkå¯ç”¨ï¼Œä½¿ç”¨å®ƒè¿›è¡ŒåŸºå‡†æµ‹è¯•
        try:
            import pytest_benchmark
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„åŸºå‡†æµ‹è¯•ä»£ç 
            print("\nåŸºå‡†æµ‹è¯•åŠŸèƒ½å¯ç”¨")
        except ImportError:
            # æ‰‹åŠ¨è¿›è¡ŒåŸºå‡†æµ‹è¯•
            print("\næ‰‹åŠ¨åŸºå‡†æµ‹è¯•:")
            
            # ç®€å•å­˜å‚¨è¿‡ç¨‹åŸºå‡†
            simple_times = []
            for _ in range(20):
                start = time.time()
                result = analyze_simple()
                end = time.time()
                simple_times.append(end - start)
                assert result is not None
            
            # ä¸­ç­‰å¤æ‚åº¦å­˜å‚¨è¿‡ç¨‹åŸºå‡†
            medium_times = []
            for _ in range(20):
                start = time.time()
                result = analyze_medium()
                end = time.time()
                medium_times.append(end - start)
                assert result is not None
            
            simple_avg = statistics.mean(simple_times)
            medium_avg = statistics.mean(medium_times)
            
            print(f"ç®€å•å­˜å‚¨è¿‡ç¨‹å¹³å‡æ—¶é—´: {simple_avg:.4f}ç§’")
            print(f"ä¸­ç­‰å­˜å‚¨è¿‡ç¨‹å¹³å‡æ—¶é—´: {medium_avg:.4f}ç§’")
            print(f"å¤æ‚åº¦æ¯”ä¾‹: {medium_avg / simple_avg:.2f}x")
            
            # æ€§èƒ½å›å½’æ£€æµ‹
            assert simple_avg < 0.5, f"ç®€å•å­˜å‚¨è¿‡ç¨‹åˆ†ææ—¶é—´å›å½’: {simple_avg}ç§’"
            assert medium_avg < 2.0, f"ä¸­ç­‰å­˜å‚¨è¿‡ç¨‹åˆ†ææ—¶é—´å›å½’: {medium_avg}ç§’" 